################################################################################
# Host-specific Hadoop 2.x configuration
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# Customize Peel configuration values appearing in
#
#   https://github.com/stratosphere/peel/blob/master/peel-extensions/src/main/resources/reference.hadoop-2.x.conf
#
# here.
#

system {
    hadoop-2 {
        format = true
        config {
            # core-site.xml entries
            core {
                fs.default.name = "hdfs://"${runtime.hostname}":45000/"
                io.file.buffer.size = 524288
                hadoop.tmp.dir = "/data/"${user.name}"/hadoop-2/tmp"
                hadoop.http.staticuser.user = ${user.name}

                # enable this if you want to use hadoop with native libraries
                # only use if there is a hadoop version compiled with native libraries for your environment!

                # enable client short circuit read
                # dfs.client.read.shortcircuit = true
                # dfs.domain.socket.path = "/data/"${user.name}"/hadoop-2/scr"
            }
            # hdfs-site.xml entries
            hdfs {
                dfs.namenode.name.dir = "/data/"${user.name}"/hadoop-2/name"
                dfs.datanode.data.dir = "/data/"${user.name}"/hadoop-2/data"
                dfs.namenode.checkpoint.dir = "/data/"${user.name}"/hadoop-2/check"
                dfs.replication = 2
                dfs.permissions.enabled = true
                dfs.blocksize = 134217728
                # namenode
                dfs.namenode.http-address = "0.0.0.0:45010"
                dfs.namenode.https-address = "0.0.0.0:45011"
                dfs.namenode.secondary.http-address = "0.0.0.0:45012"
                dfs.namenode.secondary.https-address = "0.0.0.0:45013"
                dfs.namenode.backup.address = "0.0.0.0:45014"
                dfs.namenode.backup.http-address = "0.0.0.0:45015"
                dfs.namenode.safemode.threshold-pct = "0.9f"
                dfs.namenode.safemode.extension = 10000
                dfs.namenode.handler.count = 100
                # datanode
                dfs.datanode.address = "0.0.0.0:45020"
                dfs.datanode.http.address = "0.0.0.0:45021"
                dfs.datanode.ipc.address = "0.0.0.0:45022"
                dfs.datanode.balance.bandwidthPerSec = 10000000000
                # journalnode
                dfs.journalnode.rpc-address = "0.0.0.0:45030"
                dfs.journalnode.http-address = "0.0.0.0:45031"
                dfs.journalnode.https-address = "0.0.0.0:45032"
            }

            mapred {
                # yarn
                mapreduce.framework.name = "yarn"
                mapreduce.map.java.opts = "-Xmx2g -Xms2g -Xmn200m"
                mapreduce.reduce.java.opts = "-Xmx2g -Xms2g -Xmn200m"
                mapreduce.map.memory.mb = 3072
                mapreduce.reduce.memory.mb = 3072
                mapreduce.task.io.sort.mb = 384
            }

            yarn {
                yarn.nodemanager.aux-services = "mapreduce_shuffle"
                yarn.resourcemanager.hostname = "cloud-7.dima.tu-berlin.de"
                yarn.nodemanager.disk-health-checker.min-healthy-disks = "0.0"
                yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage = "100.0"
                yarn.nodemanager.resource.memory-mb = 25600
                yarn.nodemanager.vmem-pmem-ratio = 5
            }


            # enable this if you want to use hadoop with native libraries
            # only use if there is a hadoop version compiled with native libraries for your environment!
            
            # env {
            #    HADOOP_COMMON_LIB_NATIVE_DIR = "$HADOOP_INSTALL/lib/native"
            #    HADOOP_OPTS= "-Djava.library.path="${system.hadoop-2.path.home}"/lib/native"
            # }
        }
    }
}
